# Template: test_generation
# Version: 2.0.0
# MAID Spec: v1.2

I need you to create behavioral tests that verify the artifacts defined in a MAID manifest.

## Task Context

**Manifest Path:** ${manifest_path}
**Goal:** ${goal}

## Expected Artifacts to Test

The manifest declares these artifacts that your tests must verify:

${artifacts_summary}

## Files Context

**Files being tested:** ${files_to_test}
**Test file to create:** ${test_file_path}

## Your Task

Create comprehensive behavioral tests that verify:
1. **Artifact existence** - Classes, functions, methods exist
2. **Artifact signatures** - Parameters and return types match manifest
3. **Artifact behavior** - Implementation works correctly

Use your **Write tool** to create the test file at: `${test_file_path}`

### Test File Requirements

1. **Test artifact existence:**
   - Import the module/class
   - Verify classes and functions exist
   - Check that methods are present on classes

2. **Test artifact signatures:**
   - Call functions/methods with correct parameters
   - Verify return types match manifest
   - Check parameter names are correct (use keyword arguments)

3. **Test artifact behavior:**
   - Test happy path (valid inputs)
   - Test edge cases (empty inputs, None, etc.)
   - Test error handling (invalid inputs)

4. **Follow pytest conventions:**
   - Test functions start with `test_`
   - Use clear, descriptive test names
   - One assertion per test (when possible)
   - Use fixtures for common setup

### Test Template Structure

```python
"""Behavioral tests for task-XXX: [description].

This test file verifies that the implementation matches the manifest
specifications for [brief description of what's being tested].
"""

import pytest
from module_path import ClassName, function_name


class TestClassName:
    """Tests for ClassName class."""

    def test_class_exists(self):
        """Test that ClassName class exists and can be instantiated."""
        instance = ClassName(param1="test")
        assert instance is not None
        assert isinstance(instance, ClassName)

    def test_method_exists(self):
        """Test that method_name method exists on ClassName."""
        instance = ClassName(param1="test")
        assert hasattr(instance, "method_name")
        assert callable(getattr(instance, "method_name"))

    def test_method_signature(self):
        """Test that method_name has correct signature and return type."""
        instance = ClassName(param1="test")

        # Call with keyword arguments to verify parameter names
        result = instance.method_name(arg1="value", arg2=42)

        # Verify return type matches manifest
        assert isinstance(result, dict)

    def test_method_behavior_happy_path(self):
        """Test method_name with valid inputs."""
        instance = ClassName(param1="test")
        result = instance.method_name(arg1="value", arg2=42)

        assert result["status"] == "success"
        assert result["arg1"] == "value"
        assert result["arg2"] == 42

    def test_method_behavior_edge_cases(self):
        """Test method_name with edge case inputs."""
        instance = ClassName(param1="test")

        # Test with empty string
        result = instance.method_name(arg1="", arg2=0)
        assert isinstance(result, dict)

        # Test with maximum value
        result = instance.method_name(arg1="x" * 1000, arg2=999999)
        assert isinstance(result, dict)

    def test_method_error_handling(self):
        """Test method_name error handling with invalid inputs."""
        instance = ClassName(param1="test")

        # If manifest specifies error handling, test it
        # Example: negative numbers should raise ValueError
        with pytest.raises(ValueError):
            instance.method_name(arg1="value", arg2=-1)


class TestStandaloneFunctions:
    """Tests for standalone functions."""

    def test_function_exists(self):
        """Test that function_name function exists."""
        assert hasattr(module_path, "function_name")
        assert callable(function_name)

    def test_function_signature(self):
        """Test function_name has correct signature."""
        result = function_name(param="test")
        assert isinstance(result, str)

    def test_function_behavior(self):
        """Test function_name produces correct output."""
        result = function_name(param="test")
        assert result == "processed: test"

    def test_function_with_empty_input(self):
        """Test function_name handles empty input."""
        result = function_name(param="")
        assert isinstance(result, str)
```

### Testing Guidelines

**For Classes:**
```python
def test_class_instantiation():
    """Test class can be instantiated with required parameters."""
    instance = ClassName(param1="value")
    assert instance is not None

def test_class_attributes():
    """Test class has expected attributes."""
    instance = ClassName(param1="value")
    assert hasattr(instance, "param1")
    assert instance.param1 == "value"
```

**For Functions/Methods:**
```python
def test_function_return_type():
    """Test function returns correct type."""
    result = function_name(param="test")
    assert isinstance(result, dict)  # or str, list, etc.

def test_function_return_structure():
    """Test function returns expected structure."""
    result = function_name(param="test")
    assert "success" in result
    assert "result" in result
    assert "error" in result
```

**For Error Handling:**
```python
def test_function_handles_none_input():
    """Test function handles None input gracefully."""
    result = function_name(param=None)
    assert isinstance(result, dict)
    assert result["success"] is False

def test_function_raises_on_invalid_type():
    """Test function raises TypeError for invalid input type."""
    with pytest.raises(TypeError):
        function_name(param=123)  # Should be string
```

### Example Test File from MAID Agents

```python
"""Behavioral tests for task-002: Validation runner.

This test file verifies that ValidationRunner correctly executes
maid CLI commands and pytest validation commands.
"""

import subprocess
from unittest.mock import Mock, patch

import pytest

from maid_agents.core.validation_runner import ValidationRunner


class TestValidationRunner:
    """Tests for ValidationRunner class."""

    def test_class_exists(self):
        """Test that ValidationRunner class exists."""
        runner = ValidationRunner()
        assert runner is not None
        assert isinstance(runner, ValidationRunner)

    def test_validate_manifest_method_exists(self):
        """Test that validate_manifest method exists."""
        runner = ValidationRunner()
        assert hasattr(runner, "validate_manifest")
        assert callable(runner.validate_manifest)

    @patch("subprocess.run")
    def test_validate_manifest_success(self, mock_run):
        """Test validate_manifest with successful validation."""
        # Mock successful maid validate
        mock_run.return_value = Mock(
            returncode=0,
            stdout="Validation passed",
            stderr=""
        )

        runner = ValidationRunner()
        result = runner.validate_manifest("manifests/task-001.manifest.json")

        assert result["success"] is True
        assert result["output"] == "Validation passed"
        assert result["error"] is None

        # Verify correct command was called
        mock_run.assert_called_once()
        args = mock_run.call_args[0][0]
        assert args == ["maid", "validate", "manifests/task-001.manifest.json"]

    @patch("subprocess.run")
    def test_validate_manifest_failure(self, mock_run):
        """Test validate_manifest with validation failure."""
        # Mock failed validation
        mock_run.return_value = Mock(
            returncode=1,
            stdout="",
            stderr="Error: Invalid manifest"
        )

        runner = ValidationRunner()
        result = runner.validate_manifest("manifests/invalid.manifest.json")

        assert result["success"] is False
        assert result["error"] == "Error: Invalid manifest"

    @patch("subprocess.run")
    def test_run_behavioral_tests_success(self, mock_run):
        """Test run_behavioral_tests with passing tests."""
        # Mock successful pytest run
        mock_run.return_value = Mock(
            returncode=0,
            stdout="5 passed in 0.12s",
            stderr=""
        )

        runner = ValidationRunner()
        result = runner.run_behavioral_tests(
            ["pytest", "tests/test_example.py", "-v"]
        )

        assert result["success"] is True
        assert "passed" in result["output"]
```

## Important Testing Principles

### Test Artifact USAGE, Not Implementation

❌ **Wrong** - Testing implementation details:
```python
def test_method_uses_specific_algorithm():
    """This tests HOW it works, not WHAT it does."""
    instance = MyClass()
    # Don't test internal implementation
```

✅ **Correct** - Testing artifact interface:
```python
def test_method_processes_input_correctly():
    """This tests WHAT it does through its public API."""
    instance = MyClass()
    result = instance.process(input="test")
    assert result["status"] == "processed"
```

### Verify Manifest Signatures

Every artifact in the manifest must be tested:

```python
# If manifest declares:
# {"type": "function", "name": "calculate", "args": [{"name": "x", "type": "int"}], "returns": "int"}

# Then test must verify:
def test_calculate_signature():
    """Test calculate function signature matches manifest."""
    result = calculate(x=5)  # Use keyword arg to verify param name
    assert isinstance(result, int)  # Verify return type
```

### Edge Cases to Test

1. **Empty inputs:** `""`, `[]`, `{}`
2. **None values:** Test None handling
3. **Boundary values:** Min/max integers, empty/full collections
4. **Invalid types:** Wrong type parameters (should raise TypeError or handle gracefully)
5. **Invalid values:** Negative numbers where positive expected, etc.

## Common Mistakes to Avoid

- ❌ **Testing implementation details** instead of public interface
- ❌ **Missing existence tests** for classes/functions
- ❌ **Not using keyword arguments** (can't verify parameter names)
- ❌ **Forgetting to test return types** (use `isinstance()`)
- ❌ **Not testing edge cases** (empty inputs, None, etc.)
- ❌ **Not testing error handling** (use `pytest.raises()`)
- ❌ **Importing from wrong module path**

## File Access Boundaries

For this test generation phase, you should:
- **Read:** The manifest file to understand what to test
- **Read (optional):** The implementation files to understand context
- **Write:** The test file at `${test_file_path}`
- **Do NOT modify:** The manifest or implementation files

This ensures proper isolation and traceability in the MAID methodology.

## Next Steps

1. Review the manifest artifacts that need testing
2. Design tests that verify existence, signatures, and behavior
3. Use your Write tool to create the test file
4. Include comprehensive test coverage for all artifacts
5. Confirm the test file was created successfully

Please proceed with creating the behavioral tests!
